import numpy as np
import os
from sklearn.manifold import TSNE
from sklearn.decomposition import PCA

# Directorio de salida
output_path = "data/reduction"
os.makedirs(output_path, exist_ok=True)

# Diccionario con los conjuntos de datos y sus rutas
datasets = {
    "train": "data/r2plus1d_18_16_kinetics_train/features_train.npy",
    "val": "data/r2plus1d_18_16_kinetics_val/features_val.npy"
}

# Par√°metros
num_samples = 10000  # N√∫mero de muestras despu√©s de subsampling
pca_components = 50  # Reducimos a 50 dimensiones

# Aplicar PCA y t-SNE a cada conjunto de datos
for name, path in datasets.items():
    print(f" Procesando {name}...")

    # Cargar datos
    features = np.load(path, allow_pickle=True)
    
    # Subsampling aleatorio si el dataset es grande
    if features.shape[0] > num_samples:
        idx = np.random.choice(features.shape[0], num_samples, replace=False)
        features = features[idx]

    # Aplicar PCA para reducir de 512 a 50 dimensiones
    print(f" Aplicando PCA a {name}...")
    pca = PCA(n_components=pca_components, random_state=42)
    features_pca = pca.fit_transform(features)

    # Aplicar t-SNE 2D
    print(f" Aplicando t-SNE 2D a {name}...")
    tsne_2d = TSNE(n_components=2, random_state=42, perplexity=50, n_iter=500, method='barnes_hut', learning_rate=200)
    features_tsne_2d = tsne_2d.fit_transform(features_pca)
    np.save(os.path.join(output_path, f"{name}_tsne_2d.npy"), features_tsne_2d)

    # Aplicar t-SNE 3D
    print(f" Aplicando t-SNE 3D a {name}...")
    tsne_3d = TSNE(n_components=3, random_state=42, perplexity=50, n_iter=500, method='barnes_hut', learning_rate=200)
    features_tsne_3d = tsne_3d.fit_transform(features_pca)
    np.save(os.path.join(output_path, f"{name}_tsne_3d.npy"), features_tsne_3d)

    print(f"Reducci√≥n con t-SNE completada para {name} y guardada en {output_path}")

print("Proceso finalizado.")

# üîπ Justificaci√≥n de los ajustes:
# 1Ô∏è.- Aplicamos **PCA** antes de t-SNE para reducir las dimensiones de 512 a 50.
#    Esto mejora la eficiencia computacional y mantiene la mayor parte de la informaci√≥n relevante.
# 2Ô∏è.- Usamos **subsampling** (10,000 muestras en lugar de todo el dataset) para reducir el costo computacional.
#    - t-SNE tiene una complejidad de O(N¬≤), por lo que reducir el n√∫mero de muestras disminuye el tiempo de ejecuci√≥n significativamente.
#    - Con 10,000 muestras, a√∫n capturamos una buena representaci√≥n de la distribuci√≥n original.
# 3Ô∏è.- Ajustamos los **par√°metros de t-SNE** para un balance entre velocidad y calidad:
#    - `perplexity=50`: Optimizado para datasets grandes.
#    - `n_iter=500`: Reduce iteraciones para acelerar el proceso.
#    - `method='barnes_hut'`: Mejora la eficiencia en grandes vol√∫menes de datos.
#    - `learning_rate=200`: Mantiene estabilidad en la reducci√≥n.
#
# üîπ Con estos cambios, reducimos el tiempo de ejecuci√≥n de **varias horas** a **20-30 minutos** sin comprometer demasiado la calidad de la representaci√≥n.
